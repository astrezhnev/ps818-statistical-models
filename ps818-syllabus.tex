\documentclass[11pt, article, oneside]{memoir}

%% Required packages
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx, url}
\usepackage{rotating}
\usepackage{multicol}
\usepackage[small,it]{caption}
\usepackage{subfig}
%\usepackage{fullpage, setspace}
\usepackage{epigraph}
\usepackage[all]{xy}
\usepackage{verbatim}
\usepackage[authordate, backend=biber, doi=false, isbn=false,
            backref=true, maxbibnames=10, hyperref=true,
            dateabbrev=false, uniquename=false]{biblatex-chicago}


%% For putting floats at the end
%\usepackage[nolists]{endfloat}


%% XeLaTeX packages + options
\usepackage{mathspec}
\usepackage{xunicode}
\defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase,Numbers=OldStyle}
%\setsansfont{Helvetica Neue Light}
%\setsansfont{Quicksand}
\setsansfont{Linux Biolinum O}
\setmainfont{Linux Libertine O}
\setmonofont{Inconsolata}
\setmathrm{Linux Libertine O}
\setmathsfont(Latin)[Uppercase=Italic, Lowercase=Italic,
Kerning=Off]{Linux Libertine O}
\setmathsfont(Greek)[Uppercase=Regular, Lowercase=Regular]{Linux Libertine O}
\setmathsfont(Digits)[Numbers={Lining,Proportional}]{Linux Libertine O}


%% Tikz for drawing figures
\usepackage{pgf, tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}


%% Fancy section labels
\usepackage[compact]{titlesec}
\titleformat{\section}[hang]{\Large\sffamily\bfseries}{\S{\addfontfeatures{Numbers=OldStyle}\thesection}}{1em}{}{}
\titleformat{\subsection}[hang]{\large\sffamily\bfseries}{\addfontfeatures{Numbers=OldStyle}\thesubsection}{1em}{}
\titlespacing*{\section}{0em}{1.5em}{0.5em}
\titlespacing*{\subsection}{0em}{1.5em}{0.5em}


\newcommand{\vs}{\vspace{-\baselineskip}}
\theoremstyle{Assumption}
\newtheorem{assump}{Assumption}
\newcommand{\indep}{\perp\!\!\!\perp}

%% Bibliography files
%\addbibresource{mb.bib}
%\addbibresource{gk.bib}
%\addbibresource{gkpubs.bib}

%% Put url links in titles of bibliography
\ExecuteBibliographyOptions{url=false}
\ExecuteBibliographyOptions{doi=false}
\newbibmacro{string+url}[1]{%
 \iffieldundef{doi}{\iffieldundef{url}{#1}{\href{\thefield{url}}{#1}}}{\href{http://dx.doi.org/\thefield{doi}}{#1}}}
\DeclareFieldFormat{title}{\usebibmacro{string+url}{\mkbibemph{#1}}}
\DeclareFieldFormat[article]{title}{\usebibmacro{string+url}{\mkbibquote{#1}}}
\DeclareFieldFormat[misc]{title}{\usebibmacro{string+url}{\mkbibemph{#1}}}
\DeclareFieldFormat[book]{title}{\usebibmacro{string+url}{\mkbibemph{#1}}}

%% Name, Title, Affiliation, Contact. Change as needed.
\def\myaffiliation{Department of Political Science, University of Wisconsin-Madison}
\def\myauthor{Anton Strezhnev}
\def\myemail{\texttt{\href{mailto:strezhnev@wisc.edu}{strezhnev@wisc.edu}}}
\def\mywebsite{\mbox{\url{http://www.antonstrezhnev.com}}}
\def\myaddress{Pick Hall 328, 3rd floor, 5828 S University Ave}
\def\mytitle{PS 818: Data Analysis with Statistical Models (``MLE")}
\def\mykeywords{Anton Strezhnev, Statistical Models}


%% Custom colors
\definecolor{gray}{rgb}{0.459,0.438,0.471}
\definecolor{crimson}{rgb}{0.34, 0.18, 0.55}


%% Create a command to make a note at the top of the first page describing the
%% publication status of the paper. 
\newcommand{\published}[1]{% 
   \gdef\puB{#1}} 
   \newcommand{\puB}{} 
   \renewcommand{\maketitlehooka}{% 
       \par\noindent\small \puB} 

\usepackage[plainpages=false, 
            pdfpagelabels, 
            bookmarksnumbered,
            pdftitle={\mytitle}, 
            pdfauthor={\myauthor},
            pdfkeywords={\mykeywords},
            colorlinks=true,
            citecolor=crimson, 
            linkcolor=crimson, 
            urlcolor=crimson]{hyperref} 
% \makeatletter
% \newcommand\org@hypertarget{}
% \let\org@hypertarget\hypertarget
% \renewcommand\hypertarget[2]{%
% \Hy@raisedlink{\org@hypertarget{#1}{}}#2%
% } \makeatother


%% blank label items; hanging bibs for text
%% Custom hanging indent for vita items
\def\ind{\hangindent=1 true cm\hangafter=1 \noindent}
\def\labelitemi{$\cdot$}

    % Title flush left
    \pretitle{\begin{flushleft}\Huge\sffamily}
    \posttitle{\end{flushleft}\par\vskip 0.5em}
    \preauthor{\begin{flushleft}\sffamily  \Large \vspace{0.25em}}
    \postauthor{\end{flushleft}}
    \predate{\begin{flushleft}\sffamily \small\vspace{0.9em}}
    \postdate{\end{flushleft}\par\vskip 2em}

\title{{\mytitle}}

\author{\myauthor\smallskip\footnotesize\newline Office: North Hall 322D, 3rd floor
  \newline Office Hours: Drop in or schedule an appointment by e-mail \newline
    \myemail \newline \mywebsite
\newline
}

\published{{\sffamily Fall 2025 | Lecture: Mon./Wed.,  9:30am-10:45am, | Room: Ingraham Hall 223 (Lecture) | Units: 3}}

\counterwithout{section}{chapter}

\date{}
\begin{document}
\maketitle
\textbf{Last Updated: 08/26/2025}
\section*{Course Overview}

Statistical models provide a structure for the analysis of data. Often many scientific questions revolve around drawing statistical inferences about some parameter, such as a regression coefficient. Alternatively, models provide a structured way for generating predictions on new or out-of-sample data. Understanding the fundamentals of how to define, estimate and validate a statistical model is essential to the process of quantitative empirical research.

This course is part of the second year of the quantitative methods sequence and builds on the material covered in the first year. It will introduce students to likelihood and Bayesian inference with a focus on multilevel/hierarchical regression models. The overarching framework of this class is model-based inference for description and prediction -- a complement to the design-based framework of causal inference. We will discuss, however, how to incorporate predictive models into a causal inference workflow, particularly when adjusting for confounding in selection-on-observables designs. Students will learn both the theory behind Bayesian modeling as well as how to implement common estimators (e.g. Expectation-Maximization, Markov Chain Monte Carlo (MCMC)) in the R statistical programming language. They will also be exposed to modern machine learning methods for flexible estimation of regression models -- techniques such as random forests and kernel methods/gaussian processes among others. Applied examples will be drawn from across the political science literature, with a particular emphasis on the analysis of large survey data (e.g. the American National Election Survey (ANES), the Cooperative Election Survey (CES), the European Social Survey (ESS)).

This course will involve a combination of lectures and problem sets. Lectures will focus on introducing the core theoretical concepts being taught in this course as well as providing illustrations through worked applied examples. Problem sets will contain a mixture of both theoretical and applied questions and serve to reinforce key concepts and allow students to assess their progress and understanding throughout the course. Primary evaluation will take the form of a take-home midterm and an in-person final exam.

Assignments will involve analysis of data using the R programming language. This is a free and open source language for statistical computing that is used extensively for data analysis in many fields. Prior experience with the fundamentals of R programming is required. Assignments will be written and distributed using the ``Quarto" publishing system -- the most recent extension of the ``R Markdown" notebook interface. 

\section*{Prerequisites}

This course assumes that you have both a background in the core concepts of probability, statistics and inference as well as prior exposure to linear regression models. Completing the first two courses in the political science graduate methodology sequence should prepare you for the material in this class. However, there are no strict, specific course pre-requisites as many different disciplines and departments offer introductory statistics classes that cover the relevant material.

If you are unsure of whether you meet the requirements, skim/read through the first six chapters of \textit{Regression and Other Stories}, one of the books being used by this course. You should find most of the concepts behind the material relatively familiar, aside from the references to Bayesian models (which will be covered in this course).

Please contact the instructor at (\href{mailto:strezhnev@wisc.edu}{strezhnev@wisc.edu}) if you are interested in enrolling but are unsure of the requirements. 

\section*{Logistics}

\textbf{Lectures}: Monday/Wednesday  9:30am-10:45am -- Location: Ingraham Hall 223
\newline\newline \textbf{Disucssion Forum:} We will use the Ed platform as a course discussion board. See the Canvas page for more details.
\newline\newline\textbf{Course Materials}: Lecture materials, assignments and section code will be posted on the course GitHub page at \url{https://github.com/astrezhnev/ps-818-statistical-models/}.

Readings will be listed on the syllabus. I will also post links to any non-textbook readings on the Modules page on Canvas.

\section*{Textbooks} 

The course will involve readings from a variety of different textbook chapters and published papers. The class will not require the purchase of any textbook as they are available online. However, you may wish to obtain a paper copy for your own personal use or reference.

Two textbooks from which many readings will be drawn are:

\begin{itemize}
\item Gelman, A., Hill, J., \& Vehtari, A. (2020). \textit{Regression and other stories.} Cambridge University Press. (An introduction to regression and multilevel modeling from an applied perspective) \url{https://avehtari.github.io/ROS-Examples/index.html}
\item Gelman, A., Carlin, J., Stern H., Dunson, D., Vehtari A., \& Rubin, D. (2013). \textit{Bayesian Data Analysis.} 3rd Edition. Chapman and Hall/CRC. (A more advanced text on Bayesian modeling) \url{http://www.stat.columbia.edu/~gelman/book/}
\end{itemize}


\section*{Requirements}

Studentsâ€™ final grades are based on three components:
\begin{itemize}

\item \textbf{Problem sets} (25\% of the course grade). Students will complete a total of four problem sets throughout the semester. Problem sets will primarily cover topics from the lecture and section for that week and the previous week.

The goal of the problem sets is to encourage exploration of the material and to provide you with a clear and credible means of assessing your understanding and progress through the course.

Problem sets will be graded on a (+/\checkmark/-) scale with a + awarded for complete and near-perfect work, a \checkmark awarded for generally good work with clear effort shown but with some errors, and a - awarded for significantly incomplete work with major conceptual errors and little effort shown.
 
\begin{itemize}
       \item \textit{Collaboration policy}: I strongly encourage collaboration between students on the problem sets and highly recommend that students discuss problems with each other either in person or via Ed. However, each student is expected to submit their own write-up of the answers and any relevant code. 
        \item \textit{Office hours and online discussion}: Students should feel free to discuss any questions about the problem sets with me during class and during office hours. I also strongly encourage students to post questions about both the problem sets and the readings on the course Ed board and respond to other studentsâ€™ questions. Responding to other studentsâ€™ questions will contribute to your participation grade.
        \item \textit{Submission guidelines}: Problem sets will be distributed as \texttt{HTML} and \texttt{Quarto} files (\texttt{.qmd}). You should submit your answers and any relevant R code in the same format: including a \texttt{Quarto} file (\texttt{.qmd} extension) and a corresponding rendered \texttt{.html} file as your submission. \texttt{Quarto} builds on the earlier \texttt{R Markdown} project which allows you to combine the text formatting syntax of Markdown markup language with the ability to embed and execute chunks of code directly into a text document. This allows you to present your code, graphical output, and discussion/write-up all in the same document. I highly recommend that you edit the distributed \texttt{Quarto} assignment file for each problem set directly to make organization easier.
        \end{itemize}
\item  \textbf{Take-home midterm and final} (25\% and 40\% of the course grade respectively). The take-home midterm and final exams will have the same format and structure as the problem sets but with one key difference. You are \textbf{not} permitted to collaborate with other students or any other individual on the exams. I will answer any clarifying questions on the \textsc{Ed} discussion board, but will not answer substantive questions.
    \item \textbf{Participation} (10\% of the course grade). I expect students to take an active role in learning in lecture. Engagement with the teaching staff by asking and answering questions will contribute to this grade as will interaction on the Ed discussion board.
\end{itemize}

\section*{Computing}

This course will use the \texttt{R} programming language. This is a free and open source programming language that is available for nearly all computing platforms. You should download and install it from \url{http://www.r-project.org}. Unless you have strong preferences for a specific coding environment, I also highly recommend that you use the free \href{https://rstudio.com}{RStudio} Desktop Integrated Development Environment (IDE) which you can download from \url{https://rstudio.com/products/rstudio/download/#download}. In addition to being a great and simple to use environment for editing code, \texttt{RStudio} makes it very easy to write and compile \texttt{Quarto} documents: the format in which problem sets will be distributed. For a guide to using Quarto, see \url{https://www.quarto.org}. In addition to base \texttt{R}, we will be frequently using data management and processing tools found in the \href{https://www.tidyverse.org/}{tidyverse} set of packages along with basic graphics and visualization using \href{https://ggplot2.tidyverse.org/}{ggplot2}. 

The course will also introduce the \texttt{Stan} language and software for specifying and estimating Bayesian models. Stan is written in C but has bindings for a variety of programming languages. We will use two interfaces for \texttt{Stan} in R: \texttt{RStan} and \texttt{brms}. Running Stan on your local machine will require that you have the correct libraries installed to allow you to compile C++ source code into an executable. See the installation guide at \url{https://mc-stan.org/install/} for more details on the requirements for your particular operating system.

\subsection*{Policy on Generative Large Language Models}

The rapid growth in both the capabilities and the accessibility of generative large language models (LLMs) such as the GPT series, DeepSeek, Gemini, Claude, etc... has introduced some novel challenges to the classroom. On the one hand, generative text models can be used as a tool to improve the quality of students' writing. On the other hand, they can be readily used to represent another's work as one's own -- that is, to commit plagiarism. Additionally, LLMs may appear to be useful for some tasks -- such as summarizing a set of texts or finding new sources on a particular topic -- when in fact the outputs are arguably sub-optimal relative to conventional research methods.
\newline\newline
\textbf{My view in short:} Large language models are marvels of \textbf{engineering}. You should use them for \textbf{engineering} tasks, but the task of research is not purely engineering and LLMs are much less effective for the task of doing \textbf{science}.
\newline\newline
By ``engineering," I mean the the iterative task of solving a problem by brainstorming potential solutions, implementing those solutions, and then subsequently \textit{evaluating} the solutions with respect to some clearly defined criteria. The key components here are both the existence of a well-defined problem and the ability to assess whether the proposed solutions are effective.
\newline\newline
Currently, the most obvious and effective use-case for large language models is in coding. I am perfectly happy for you to experiment with using LLMs in debugging code. The interactivity is great for beginning programmers who may have an idea of what they want their code to do, but are unfamiliar with the syntax of a particular language. Likewise, it's an incredibly valuable tool for experienced programmers who want to quickly generate some prototype code that is customized to their particular problem.
\newline\newline
Why is programming an ideal use case? Programming is fundamentally an engineering task. There is a clearly defined problem that a programmer needs to solve via code and there is a straightforward way to evaluate whether a block of code works. As a result, mistakes are easy to catch -- if the code throws an error, something needs to be changed. There is always a human in the loop who is capable of evaluating the output.
\newline\newline
However, I am increasingly negative on the use of LLMs for programming when one is \textit{learning} to code. In practice, I find that students delegate far too much to the model and spend insufficient time understanding the mechanics of what the code is doing. This makes it actually quite difficult to meaningfully debug outputs and understand how to diagnose errors. Additionally, actually implementing statistical methods in code is a common way for students to actually understand conceptually what the methods do and how they work. As such, I would still discourage LLM-assistance when completing problem sets although I do not strictly rule it out.
\newline\newline
Outside of coding, I do not think LLM outputs are too useful for science, especially for generating text that is to be submitted without further refinement. In general, you should be cautious about any LLM outputs that you are not able to verify or evaluate yourself. 
\newline\newline
Irrespective of whether LLM outputs are ``good" or not, it is absolutely clear that presenting LLM-generated output as one's own ideas is clearly plagiarism and will be treated as such. This does not rule out all uses of LLM-generated text, but it does rule out most. One use that I would consider acceptable is cleaning up original text that you have written to eliminate grammar mistakes or to rephrase the text to have a clearer style. We already accept the use of spellcheckers and thesauruses that are embedded in most word processors and I don't see this use case as substantively different as long as your original writing is the input. It is important, however, that you are able to evaluate the output and determine that it is conveying exactly what you want to say in exactly the way that you want to say it, just as you would when using any other writing tool.
\newline\newline
Beyond this particular use, \textbf{submitting LLM-generated text as a substitute for your own thinking is not permitted in this class and will be considered plagiarism}. This includes prompting an LLM to compose all or part of your writing and submitting that output either verbatim or with some editing. This policy also applies to generating posts on the Ed discussion board. 
\newline\newline
In general, I do not think that presently there are too many good uses for LLMs for the particular tasks that you will be doing in this class. Although these models can be utilized for things like brainstorming, summarizing text, and search - acting as something of a personalized tutor - and the quality of the model outputs does appear to be steadily growing, I think that you will find significant value to working through the course material directly and asking questions to the teaching staff and to your colleagues in the class.

\section*{Schedule}

A schedule of topics and readings is provided below. Each week will cover a single topic or group of topics. You should treat the readings as a reference and as a more detailed exposition of the topics discussed in lecture. Consult the readings when you want to know more or want a slightly different approach to explaining a particular topic.

\subsection{Week 1: Sept 3 - Course introduction and Review}

\subsection{Week 2: Sept 8-10 - Introduction to Likelihood Inference}

\begin{itemize}
  \item What are statistical models good for?
  \item What is a ``parametric" model?
  \item The likelihood function 
  \item Maximum likelihood estimation
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item \textbf{Review:} ``Regression and Other Stories" - Chapters 1-7
\end{itemize}

\textbf{Problem Set 1 Assigned September 9, Due September 22}

\subsection{Week 3: Sept 15-17 - Generalized Linear Models}

\begin{itemize}
\item Properties of maximum likelihood estimators
\item Binary outcome models,
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item ``Regression and Other Stories`` - Chapters 13-14
\end{itemize}

\subsection{Week 4: Sept 22-24 - More Likelihood Models}

\begin{itemize}
\item Duration models
\item Event Count Models
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Box-Steffensmeier, J. M., \& Jones, B. S. (1997). Time is of the essence: Event history models in political science. \textit{American Journal of Political Science}, 1414-1461.
\item Wooldridge, J. M. (1999). Chapter 8: ``Quasi-likelihood methods for count data." In \textit{Handbook of applied econometrics}, 2, 35-406.
\end{itemize}


\subsection{Week 5: Sept 29-Oct 1 - Bayesian Inference}

\begin{itemize}
\item Principles of posterior inference
\item How to write a bayesian model
\item Quantities of interest: Posterior Mode, Posterior Mean, Credible Intervals
\item Estimation and inference via Markov Chain Monte Carlo
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item ``Regression and Other Stories``: Chapter 9
\item ``Bayesian Data Analysis`` Chapters 10-11
\end{itemize}

\textbf{Problem Set 2 Assigned September 29, Due October 13}

\subsection{Week 6: Oct 6-8 - Multilevel regression models}

\begin{itemize}
\item ``Hierarchical" regression models -- random slopes/random intercept models
\item Estimation via MCMC in \texttt{Stan}
\item Interpreting and analyzing results
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Park, David K., Andrew Gelman, and Joseph Bafumi. "Bayesian multilevel estimation with poststratification: State-level estimates from national polls." Political Analysis 12.4 (2004): 375-385.
\item ``Regression and Other Stories``: Chapter 9, 11, Appendix A
\item ``Bayesian Data Analysis`` Chapter 15
\end{itemize}

\subsection{Week 7: Oct 13-15 - Predicting from models}

\begin{itemize}
\item Out-of-sample prediction and model validation
\item Estimating useful quantities of interest
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Rohrer, J. M., \& Arel-Bundock, V. (2025, August 25). Models as Prediction Machines: How to Convert Confusing Coefficients into Clear Quantities. 
\end{itemize}

\textbf{Midterm Exam: Assigned October 7, Due October 13}

\subsection{Week 8: Oct. 20-22 - Working with survey data}

\begin{itemize}
\item How to approach population inference from non-probability samples: constructing and using weights
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Caughey, D., Berinsky, A. J., Chatfield, S., Hartman, E., Schickler, E., \& Sekhon, J. S. (2020). Target estimation and adjustment weighting for survey nonresponse and sampling bias. Cambridge University Press.
\item Hanretty, Chris. "An introduction to multilevel regression and post-stratification for estimating constituency opinion." Political Studies Review 18.4 (2020): 630-645.
\end{itemize}

\subsection{Week 9: Oct. 27-29 - Mixture Models and the EM Algorithm}

\begin{itemize}
\item Exploratory data analysis and clustering models
\item MLE and MAP estimation via the ``Expectation-Maximization" algorithm
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Imai, Kosuke, and Dustin Tingley. "A statistical method for empirical testing of competing theories." American Journal of Political Science 56.1 (2012): 218-236.
\item McLachlan, Geoffrey J., Sharon X. Lee, and Suren I. Rathnayake. "Finite mixture models." Annual review of statistics and its application 6 (2019): 355-378.
\item "Bayesian Data Analysis" Chapters 13, 22
\end{itemize}

\textbf{Problem Set 3 Assigned October 28, Due November 10}

\subsection{Week 10: Nov. 3-5 - Item Response Theory and Ideal Point Models}

\begin{itemize}
\item Latent variable models from a bayesian perspective
\item ``Ideal point" models for voting
\item Extensions to models of networks 
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Clinton, Joshua, Simon Jackman, and Douglas Rivers. "The statistical analysis of roll call data." American Political Science Review 98, no. 2 (2004): 355-370.
\item Treier, Shawn, and Simon Jackman. "Democracy as a latent variable." American Journal of Political Science 52, no. 1 (2008): 201-217.
\item Martin, Andrew D., and Kevin M. Quinn. "Dynamic ideal point estimation via Markov chain Monte Carlo for the US Supreme Court, 1953â€“1999." Political analysis 10, no. 2 (2002): 134-153.
\item Burkner, Paul-Christian. "Bayesian item response modeling in R with brms and Stan." arXiv preprint arXiv:1905.09501 (2019).
\end{itemize}


\subsection{Week 11: Nov. 10-12: Regularization and Model Selection}

\begin{itemize}
\item Variable selection and penalized regression (Ridge, LASSO) 
\item Cross-fitting and out-of-sample validation
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Chapter 6: James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: Springer, 2013.
\item Chapter 7: James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: Springer, 2013.
\item Stanescu, Diana, Erik Wang, and Soichiro Yamauchi. "Using LASSO to assist imputation and predict child well-being." Socius 5 (2019): 2378023118814623.
\end{itemize}

\textbf{Problem Set 4 Assigned November 11, Due December 1}

\subsection{Week 12: Nov. 17-19: Flexible Regression - Trees and Forests}

\begin{itemize}
\item Random forests
\item ``Causal forests" for effect heterogeneity
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Chapter 6: Hastie, T., Tibshirani, R., Friedman, J. H., \& Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction 
\item Chapter 15: Hastie, T., Tibshirani, R., Friedman, J. H., \& Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction
\item Chapter 8: James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: Springer, 2013.
\end{itemize}

\subsection{Week 13: Nov. 24-26: Flexible Regression - Kernels and Gaussian Processes}

\begin{itemize}
\item Kernel ridge regression
\item Gaussian process regression
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item TBA
\end{itemize}

\subsection{Week 14: Dec. 1-3: Semiparametric and Nonparametric estimation}

\begin{itemize}
\item Efficient estimation for causal inference
\item ``Doubly-robust" estimators
\item Influence functions
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Kennedy, Edward H. "Semiparametric doubly robust targeted double machine learning: a review." Handbook of statistical methods for precision medicine (2024): 207-236.
\end{itemize}

\subsubsection{Week 15: Dec. 8-10 - Big, Fast Regressions}

\begin{itemize}
\item Stochastic gradient descent
\item Efficient data compression
\end{itemize}

\begin{itemize}
\item Wong, Jeffrey, Eskil Forsell, Randall Lewis, Tobias Mao, and Matthew Wardrop. "You Only Compress Once: Optimal Data Compression for Estimating Linear Models." arXiv preprint arXiv:2102.11297 (2021).
\item Lal, Apoorva, Alexander Fischer, and Matthew Wardrop. "Large Scale Longitudinal Experiments: Estimation and Inference." arXiv preprint arXiv:2410.09952 (2024).
\end{itemize}


\textbf{Final Exam: TBA}

\section*{Assignment Schedule}

\begin{itemize}
\item Problem Set 1: Assigned September 9, Due September 22
\item Problem Set 2: Assigned September 29, Due October 13
\item \textbf{Midterm Exam}:  Assigned October 14, Due October 20 (1 week)
\item Problem Set 3: Assigned October 28, Due November 10
\item Problem Set 4: Assigned November 11, Due December 1 (Extra time due to Thanksgiving)
\item \textbf{Final Exam}: Date TBA
\end{itemize}

\section*{Acknowledgments}

This course is indebted to the many wonderful and generous scholars who have developed causal inference curricula in political science departments throughout the world and who have made their course materials available to the public. This course in particular has been heavily inspired by Gov 2001 and Gov 2003 at Harvard University as well as Quant III at MIT. In particular, I thank Matthew Blackwell, Brandon Stewart, Erin Hartman, Molly Roberts, Kosuke Imai, Teppei Yamamoto, Jens Hainmueller, Adam Glynn, Gary King, Justin Grimmer, and In Song Kim whose lecture notes and syllabi have been immensely valuable in the creation of this course. 


\end{document}
 
